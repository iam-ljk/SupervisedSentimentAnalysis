{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "from torch import flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "# import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_0_path = \"/home3/luharj/DLNLP/Assignment1/data/train/ClassificationDataset-train0.xlsx\"\n",
    "valid_data_0_path = \"/home3/luharj/DLNLP/Assignment1/data/valid/ClassificationDataset-valid0.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_0 = pd.read_excel(train_data_0_path)\n",
    "valid_df_0 = pd.read_excel(valid_data_0_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module) :\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(3,300), padding=0, stride=1)\n",
    "        self.conv2 = nn.Conv2d(1, 16, kernel_size=(4,300), padding=0, stride=1)\n",
    "        self.conv3 = nn.Conv2d(1, 16, kernel_size=(5,300), padding=0, stride=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=(68), stride=2)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=(67), stride=2)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=(66), stride=2)\n",
    "        self.fc1 = nn.Linear(in_features=16*3,out_features=16)\n",
    "        self.fc2 = nn.Linear(in_features=16,out_features=8)\n",
    "        self.fc3 = nn.Linear(in_features=8,out_features=3)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "    def forward(self, x) :\n",
    "        x1 = self.relu(self.conv1(x))\n",
    "        x1 = x1.squeeze()\n",
    "        x1 = self.pool1(x1)\n",
    "        x1 = flatten(x1, 1)\n",
    "        \n",
    "        x2 = self.relu(self.conv2(x))\n",
    "        x2 = x2.squeeze()\n",
    "        x2 = self.pool2(x2)\n",
    "        x2 = flatten(x2, 1)\n",
    "        \n",
    "        x3 = self.relu(self.conv3(x))\n",
    "        x3 = x3.squeeze()\n",
    "        x3 = self.pool3(x3)\n",
    "        x3 = flatten(x3, 1)\n",
    "        \n",
    "        x = torch.cat((x1, x2, x3), dim=1)\n",
    "        x = flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(word) :\n",
    "    if word == \"negative\" :\n",
    "        return 0\n",
    "    if word == \"neutral\" :\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(sentences) :\n",
    "    words = sentences.split(\" \")\n",
    "    embeddings = []\n",
    "    for word in words:\n",
    "        if word in glove_model:\n",
    "            embeddings.append(glove_model[word])\n",
    "        else :\n",
    "            embeddings.append(np.random.uniform(-1,1,300))\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshaped_data(data) :\n",
    "    desired_shape = (70,300)\n",
    "    rows_to_copy = min(len(data), 70)\n",
    "    padded_data = np.zeros(desired_shape)\n",
    "    padded_data[:rows_to_copy, :] = data[:rows_to_copy, :]\n",
    "    return np.array(padded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_frame) :\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    for index, row in data_frame.iterrows() :\n",
    "        data = get_embedding(row[1])\n",
    "        labels = get_labels(row[0])\n",
    "        training_data.append(reshaped_data(data))\n",
    "        training_labels.append(labels)\n",
    "    return np.array(training_data), np.array(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, training_labels = get_data(data_frame=train_df_0)\n",
    "validation_data, validation_labels = get_data(data_frame=valid_df_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.Tensor(training_data)\n",
    "y = torch.LongTensor(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X\n",
    "y_train = y\n",
    "X_val = torch.Tensor(validation_data)\n",
    "y_val = torch.LongTensor(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.unsqueeze(1)\n",
    "X_val = X_val.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2798, 1, 70, 300]) torch.Size([1203, 1, 70, 300])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 300), stride=(1, 1))\n",
       "  (conv2): Conv2d(1, 16, kernel_size=(4, 300), stride=(1, 1))\n",
       "  (conv3): Conv2d(1, 16, kernel_size=(5, 300), stride=(1, 1))\n",
       "  (relu): ReLU()\n",
       "  (pool1): MaxPool1d(kernel_size=68, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool2): MaxPool1d(kernel_size=67, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool3): MaxPool1d(kernel_size=66, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=48, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=8, bias=True)\n",
       "  (fc3): Linear(in_features=8, out_features=3, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 20  # You can adjust the number of epochs\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.9653283818201586, Accuracy: 0.6054324517512509, Macro F1: 0.2566209001852566, Micro F1: 0.6054324517512509\n",
      "Epoch 2/20, Loss: 0.8518708524378863, Accuracy: 0.7022873481057899, Macro F1: 0.3902433006035568, Micro F1: 0.7022873481057899\n",
      "Epoch 3/20, Loss: 0.7995099696246061, Accuracy: 0.7494639027877055, Macro F1: 0.45932550435773195, Micro F1: 0.7494639027877055\n",
      "Epoch 4/20, Loss: 0.7332020618698813, Accuracy: 0.82987848463188, Macro F1: 0.5401056660757325, Micro F1: 0.82987848463188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Loss: 0.6637921536510641, Accuracy: 0.8995711222301644, Macro F1: 0.6001626960783106, Micro F1: 0.8995711222301644\n",
      "Epoch 6/20, Loss: 0.622836249795827, Accuracy: 0.9342387419585418, Macro F1: 0.6289709490488528, Micro F1: 0.9342387419585418\n",
      "Epoch 7/20, Loss: 0.6074977029453624, Accuracy: 0.9481772694781987, Macro F1: 0.6395717235409965, Micro F1: 0.9481772694781987\n",
      "Epoch 8/20, Loss: 0.5985553847117857, Accuracy: 0.9538956397426733, Macro F1: 0.643520408412482, Micro F1: 0.9538956397426733\n",
      "Epoch 9/20, Loss: 0.5948514640331268, Accuracy: 0.956397426733381, Macro F1: 0.6458258900920654, Micro F1: 0.9563974267333809\n",
      "Epoch 10/20, Loss: 0.593230664730072, Accuracy: 0.9578270192994996, Macro F1: 0.6459654829673805, Micro F1: 0.9578270192994996\n",
      "Epoch 11/20, Loss: 0.5919714989987287, Accuracy: 0.9592566118656183, Macro F1: 0.647782565181657, Micro F1: 0.9592566118656183\n",
      "Epoch 12/20, Loss: 0.5903777805241671, Accuracy: 0.9606862044317369, Macro F1: 0.6481110490408158, Micro F1: 0.9606862044317369\n",
      "Epoch 13/20, Loss: 0.5898999355056069, Accuracy: 0.9610436025732666, Macro F1: 0.6489101541733121, Micro F1: 0.9610436025732666\n",
      "Epoch 14/20, Loss: 0.5894529697569934, Accuracy: 0.9610436025732666, Macro F1: 0.6487355885665934, Micro F1: 0.9610436025732666\n",
      "Epoch 15/20, Loss: 0.5884109478105198, Accuracy: 0.9624731951393852, Macro F1: 0.6501082550643321, Micro F1: 0.9624731951393852\n",
      "Epoch 16/20, Loss: 0.5883120921525088, Accuracy: 0.9624731951393852, Macro F1: 0.6498465420089704, Micro F1: 0.9624731951393852\n",
      "Epoch 17/20, Loss: 0.5881343470378355, Accuracy: 0.9624731951393852, Macro F1: 0.6504605894397932, Micro F1: 0.9624731951393852\n",
      "Epoch 18/20, Loss: 0.5882836255160245, Accuracy: 0.9624731951393852, Macro F1: 0.6497597862205762, Micro F1: 0.9624731951393852\n",
      "Epoch 19/20, Loss: 0.5881019668145613, Accuracy: 0.9628305932809149, Macro F1: 0.6506492347507921, Micro F1: 0.9628305932809149\n",
      "Epoch 20/20, Loss: 0.5877744243903593, Accuracy: 0.9631879914224446, Macro F1: 0.6506625343697752, Micro F1: 0.9631879914224446\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        output_labels = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        # Append true and predicted labels\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predicted_labels.extend(output_labels.cpu().numpy())\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    \n",
    "    # Calculate macro and micro F1 scores for the epoch\n",
    "    macro_f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "    micro_f1 = f1_score(true_labels, predicted_labels, average='micro')\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader)}, Accuracy: {accuracy}, Macro F1: {macro_f1}, Micro F1: {micro_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 300), stride=(1, 1))\n",
       "  (conv2): Conv2d(1, 16, kernel_size=(4, 300), stride=(1, 1))\n",
       "  (conv3): Conv2d(1, 16, kernel_size=(5, 300), stride=(1, 1))\n",
       "  (relu): ReLU()\n",
       "  (pool1): MaxPool1d(kernel_size=68, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool2): MaxPool1d(kernel_size=67, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool3): MaxPool1d(kernel_size=66, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=48, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=8, bias=True)\n",
       "  (fc3): Linear(in_features=8, out_features=3, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "predicted_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, labels in val_loader:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "    output_labels = torch.argmax(outputs, dim = 1)\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "    predicted_labels.extend(output_labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = accuracy_score(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_macro_f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "test_micro_f1 = f1_score(true_labels, predicted_labels, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7522859517871987\n",
      "Test Macro F1: 0.4840490653786936\n",
      "Test Micro F1: 0.7522859517871987\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test Macro F1: {test_macro_f1}\")\n",
    "print(f\"Test Micro F1: {test_micro_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 300), stride=(1, 1))\n",
       "  (conv2): Conv2d(1, 16, kernel_size=(4, 300), stride=(1, 1))\n",
       "  (conv3): Conv2d(1, 16, kernel_size=(5, 300), stride=(1, 1))\n",
       "  (relu): ReLU()\n",
       "  (pool1): MaxPool1d(kernel_size=68, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool2): MaxPool1d(kernel_size=67, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool3): MaxPool1d(kernel_size=66, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=48, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=8, bias=True)\n",
       "  (fc3): Linear(in_features=8, out_features=3, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model1.parameters(), lr=0.001)\n",
    "epochs = 20  # You can adjust the number of epochs\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.951683139259165, Accuracy: 0.5843459614010007, Macro F1: 0.31163488603598416, Micro F1: 0.5843459614010007, Validation Loss: 0.897390604019165, Val Accuracy: 0.6467165419783873, Val Macro F1: 0.2618206293117954, Val Micro F1: 0.6467165419783873\n",
      "Epoch 2/20, Train Loss: 0.8667425391348925, Accuracy: 0.6833452466047176, Macro F1: 0.3529755251680385, Micro F1: 0.6833452466047176, Validation Loss: 0.84684010242161, Val Accuracy: 0.7024106400665004, Val Macro F1: 0.3990565673747444, Val Micro F1: 0.7024106400665004\n",
      "Epoch 3/20, Train Loss: 0.8162185603922064, Accuracy: 0.7333809864188706, Macro F1: 0.43920746213719486, Micro F1: 0.7333809864188705, Validation Loss: 0.8286621727441487, Val Accuracy: 0.7273482959268496, Val Macro F1: 0.44742022975034, Val Micro F1: 0.7273482959268495\n",
      "Epoch 4/20, Train Loss: 0.7510879608717832, Accuracy: 0.8102215868477484, Macro F1: 0.5215786433752071, Micro F1: 0.8102215868477485, Validation Loss: 0.8030459943570589, Val Accuracy: 0.7547797173732336, Val Macro F1: 0.48081765976502816, Val Micro F1: 0.7547797173732336\n",
      "Epoch 5/20, Train Loss: 0.6765500550920313, Accuracy: 0.8895639742673338, Macro F1: 0.5919030443301403, Micro F1: 0.8895639742673338, Validation Loss: 0.7872639612147683, Val Accuracy: 0.7564422277639236, Val Macro F1: 0.4752960208639281, Val Micro F1: 0.7564422277639236\n",
      "Epoch 6/20, Train Loss: 0.6307670013471083, Accuracy: 0.9267333809864189, Macro F1: 0.6222958070162675, Micro F1: 0.9267333809864189, Validation Loss: 0.7859608060435245, Val Accuracy: 0.7664172901080631, Val Macro F1: 0.49304190997490194, Val Micro F1: 0.7664172901080631\n",
      "Epoch 7/20, Train Loss: 0.610307436097752, Accuracy: 0.9463902787705504, Macro F1: 0.6376594252455088, Micro F1: 0.9463902787705504, Validation Loss: 0.7868014448567441, Val Accuracy: 0.7556109725685786, Val Macro F1: 0.4890616879614831, Val Micro F1: 0.7556109725685786\n",
      "Epoch 8/20, Train Loss: 0.5991566506299105, Accuracy: 0.953180843459614, Macro F1: 0.643833793249522, Micro F1: 0.953180843459614, Validation Loss: 0.7831785741605257, Val Accuracy: 0.7597672485453034, Val Macro F1: 0.4870164712035747, Val Micro F1: 0.7597672485453034\n",
      "Epoch 9/20, Train Loss: 0.5951664068482139, Accuracy: 0.9560400285918513, Macro F1: 0.6449908101055364, Micro F1: 0.9560400285918513, Validation Loss: 0.7857942236097235, Val Accuracy: 0.7622610141313383, Val Macro F1: 0.4914570900824098, Val Micro F1: 0.7622610141313383\n",
      "Epoch 10/20, Train Loss: 0.5922979116439819, Accuracy: 0.9588992137240886, Macro F1: 0.6477612534769986, Micro F1: 0.9588992137240886, Validation Loss: 0.7888536515988802, Val Accuracy: 0.7531172069825436, Val Macro F1: 0.4872154954031355, Val Micro F1: 0.7531172069825436\n",
      "Epoch 11/20, Train Loss: 0.5903307321396741, Accuracy: 0.9606862044317369, Macro F1: 0.648186406921592, Micro F1: 0.9606862044317369, Validation Loss: 0.7832201217350206, Val Accuracy: 0.7655860349127181, Val Macro F1: 0.49236805680118323, Val Micro F1: 0.7655860349127183\n",
      "Epoch 12/20, Train Loss: 0.5882201953367754, Accuracy: 0.9628305932809149, Macro F1: 0.6509974757068443, Micro F1: 0.9628305932809149, Validation Loss: 0.7805950390665155, Val Accuracy: 0.7664172901080631, Val Macro F1: 0.4912440723345359, Val Micro F1: 0.7664172901080631\n",
      "Epoch 13/20, Train Loss: 0.5875868160616268, Accuracy: 0.9631879914224446, Macro F1: 0.6510071515403695, Micro F1: 0.9631879914224446, Validation Loss: 0.7786493301391602, Val Accuracy: 0.7655860349127181, Val Macro F1: 0.4880887556484097, Val Micro F1: 0.7655860349127183\n",
      "Epoch 14/20, Train Loss: 0.5868951949206266, Accuracy: 0.9635453895639743, Macro F1: 0.650843693790065, Micro F1: 0.9635453895639743, Validation Loss: 0.7822776625030919, Val Accuracy: 0.7655860349127181, Val Macro F1: 0.49071370051177593, Val Micro F1: 0.7655860349127183\n",
      "Epoch 15/20, Train Loss: 0.5869783203710209, Accuracy: 0.9635453895639743, Macro F1: 0.6511954942761243, Micro F1: 0.9635453895639743, Validation Loss: 0.7800872827831068, Val Accuracy: 0.7622610141313383, Val Macro F1: 0.48859774381399107, Val Micro F1: 0.7622610141313383\n",
      "Epoch 16/20, Train Loss: 0.5874658294699409, Accuracy: 0.9635453895639743, Macro F1: 0.650495757364926, Micro F1: 0.9635453895639743, Validation Loss: 0.7826291228595533, Val Accuracy: 0.7572734829592684, Val Macro F1: 0.48655148991861186, Val Micro F1: 0.7572734829592683\n",
      "Epoch 17/20, Train Loss: 0.5871126570484855, Accuracy: 0.963902787705504, Macro F1: 0.6514722776773424, Micro F1: 0.963902787705504, Validation Loss: 0.7790798984075847, Val Accuracy: 0.7639235245220283, Val Macro F1: 0.48642138163079424, Val Micro F1: 0.7639235245220283\n",
      "Epoch 18/20, Train Loss: 0.5870923643762415, Accuracy: 0.963902787705504, Macro F1: 0.6518277894335286, Micro F1: 0.963902787705504, Validation Loss: 0.7785780084760565, Val Accuracy: 0.7630922693266833, Val Macro F1: 0.4852754079379331, Val Micro F1: 0.7630922693266833\n",
      "Epoch 19/20, Train Loss: 0.586692903529514, Accuracy: 0.963902787705504, Macro F1: 0.6515607910797773, Micro F1: 0.963902787705504, Validation Loss: 0.7866692135208532, Val Accuracy: 0.7556109725685786, Val Macro F1: 0.48751582628932605, Val Micro F1: 0.7556109725685786\n",
      "Epoch 20/20, Train Loss: 0.5869725441390817, Accuracy: 0.963902787705504, Macro F1: 0.6514722776773424, Micro F1: 0.963902787705504, Validation Loss: 0.7779380616388822, Val Accuracy: 0.7655860349127181, Val Macro F1: 0.48748102562503953, Val Micro F1: 0.7655860349127183\n"
     ]
    }
   ],
   "source": [
    "# Training and validation loop\n",
    "for epoch in range(epochs):\n",
    "    model1.train()\n",
    "    running_loss = 0.0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model1(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        output_labels = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # Append true and predicted labels\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predicted_labels.extend(output_labels.cpu().numpy())\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculate accuracy, macro F1, and micro F1 on the training set\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    macro_f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "    micro_f1 = f1_score(true_labels, predicted_labels, average='micro')\n",
    "\n",
    "    # Initialize variables for validation\n",
    "    val_true_labels = []\n",
    "    val_predicted_labels = []\n",
    "    val_running_loss = 0.0\n",
    "\n",
    "    model1.eval()  # Switch to evaluation mode for validation\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "\n",
    "            val_outputs = model1(val_inputs)\n",
    "            val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "            val_output_labels = torch.argmax(val_outputs, dim=1)\n",
    "\n",
    "            # Append true and predicted labels for validation\n",
    "            val_true_labels.extend(val_labels.cpu().numpy())\n",
    "            val_predicted_labels.extend(val_output_labels.cpu().numpy())\n",
    "\n",
    "            val_running_loss += val_loss.item()\n",
    "\n",
    "    # Calculate accuracy, macro F1, and micro F1 on the validation set\n",
    "    val_accuracy = accuracy_score(val_true_labels, val_predicted_labels)\n",
    "    val_macro_f1 = f1_score(val_true_labels, val_predicted_labels, average='macro')\n",
    "    val_micro_f1 = f1_score(val_true_labels, val_predicted_labels, average='micro')\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {running_loss / len(train_loader)}, Accuracy: {accuracy}, Macro F1: {macro_f1}, Micro F1: {micro_f1}, Validation Loss: {val_running_loss / len(val_loader)}, Val Accuracy: {val_accuracy}, Val Macro F1: {val_macro_f1}, Val Micro F1: {val_micro_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
